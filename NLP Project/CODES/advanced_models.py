# -*- coding: utf-8 -*-
"""Advanced_Models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rkR6Nzo5u_VdEVO1-_HCtFZEJhq-NaxK
"""

import pandas
import numpy
import re
import nltk
import pickle
import joblib
from nltk.corpus import stopwords
nltk.download('stopwords')
from numpy import array
from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers.core import Activation, Dropout, Dense
from keras.layers import Flatten
from keras.layers import GlobalMaxPooling1D
from keras.layers import Conv1D
from keras.regularizers import l1
from keras.regularizers import l2
from keras.layers.embeddings import Embedding
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer

from google.colab import drive
drive.mount('/content/drive')

"""Importing Dataset of Tweets"""

twitter_reviews_tr = pandas.read_csv("/content/drive/My Drive/train.csv",sep=",",engine='python')
twitter_reviews_ts=pandas.read_csv("/content/drive/My Drive/test.csv",sep=",",engine='python')
#twitter_reviews.isnull().values.any()
#print(twitter_reviews.head())

"""# Preprocessing Of Data"""

TAG_REMOVAL = re.compile(r'<[^>]+>')
def tags_removal(txt):
    return TAG_REMOVAL.sub('', txt)
    
def preprocess(sentence):
    sentence_of_words = tags_removal(sentence)
    sentence_of_words = re.sub('[^a-zA-Z]', ' ', sentence_of_words)
    sentence_of_words = re.sub(r'\s+[a-zA-Z]\s+', ' ', sentence_of_words)
    sentence_of_words = re.sub(r'\s+', ' ', sentence_of_words)
    return sentence_of_words

Y_tr = twitter_reviews_tr['Stance']
targett=twitter_reviews_tr['Target']
X_tr=twitter_reviews_tr['Tweet']
m=[]
n=[]
cc=0
for a,b,c in zip(X_tr,Y_tr,targett):
  if (c=="Atheism"):
    m.append(a)
    n.append(b)
    cc+=1
print (cc)

#print (Y)
Y=[]
X=[]

nn=[]

for a in n:
  if (a=='FAVOR'):
    ll=[1,0,0]
    nn.append(ll)
  elif (a=='AGAINST'):
    ll=[0,1,0]

    nn.append(ll)
  else:
    ll=[0,0,1]
    nn.append(ll)



#Y = numpy.array(list(map(lambda x: 0 if x == 'FAVOR' elif x=='AGAINST' 1 else 2, Y)))
nn=numpy.array(nn)
sentences = list(m)
mm = []
cd=0
for sen in sentences:
    mm.append(preprocess(sen))
    cd+=1
print (cd)

#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)
Y_tes = twitter_reviews_ts['Stance']
targett2=twitter_reviews_ts['Target']
X_tes=twitter_reviews_ts['Tweet']
m1=[]
n1=[]
cc=0
for a,b,c in zip(X_tes,Y_tes,targett2):
  if (c=="Atheism"):
    m1.append(a)
    n1.append(b)
    cc+=1
print (cc)

#print (Y)
Y=[]
X=[]

nn1=[]
nn11=[]

for a in n1:
  if (a=='FAVOR'):
    ll=[1,0,0]
    nn1.append(ll)
    nn11.append(0)
  elif (a=='AGAINST'):
    ll=[0,1,0]
    nn1.append(ll)
    nn11.append(1)
  else:
    ll=[0,0,1]
    nn1.append(ll)
    nn11.append(2)



#Y = numpy.array(list(map(lambda x: 0 if x == 'FAVOR' elif x=='AGAINST' 1 else 2, Y)))
nn1=numpy.array(nn1)
sentences = list(m1)
mm1 = []
cd=0
for sen in sentences:
    mm1.append(preprocess(sen))
    cd+=1
print (cd)
X_train=mm
X_test=mm1
Y_train=nn
Y_test=nn1

tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X_train)
X_test = tokenizer.texts_to_sequences(X_test)
X_train = tokenizer.texts_to_sequences(X_train)

vocabsize = 1
vocabsize += len(tokenizer.word_index)
maxlength = 100
X_train = pad_sequences(X_train, padding='post', maxlen=maxlength)
X_test = pad_sequences(X_test, padding='post', maxlen=maxlength)

"""Using the GloVe precomputed Word Vectors"""

from numpy import array
from numpy import asarray
from numpy import zeros
glove_pretrained = open('/content/drive/My Drive/glove.6B.100d.txt', encoding="utf8")
embeddings_dict = dict()

for line in glove_pretrained:
    sen = line.split()
    word = sen[0]
    vector = asarray(sen[1 : ], dtype='float32')
    embeddings_dict[word] = vector
glove_pretrained.close()

all_words = tokenizer.word_index.items()
embedding_matrix = zeros((vocabsize, 100))
for word, index in all_words:
    embedding_vector = embeddings_dict.get(word)
    if embedding_vector is not None:
        embedding_matrix[index]=embedding_vector

"""MLP + TRAIN EMBEDDING"""

model_mlp = Sequential()
embedding_layer = Embedding(vocabsize, 100, input_length=maxlength)
model_mlp.add(embedding_layer)
model_mlp.add(Flatten())
model_mlp.add(Dense(16, activation='sigmoid'))
model_mlp.add(Dense(3, activation='softmax'))
model_mlp.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
print(model_mlp.summary())

history_mlp = model_mlp.fit(X_train, Y_train, batch_size=16000, epochs=100, verbose=1, validation_split=0.2)
score = model_mlp.evaluate(X_test, Y_test, verbose=1)
from sklearn.metrics import classification_report

y_pred = model_mlp.predict(X_test, batch_size=64, verbose=1)
y_pred_ = numpy.argmax(y_pred, axis=1)

print(classification_report(nn11, y_pred_))

scoretrain = model_mlp.evaluate(X_train, Y_train, verbose=1)
print (model_mlp.metrics_names)
print('Train Accuracy:', scoretrain[1])
print('Test Accuracy:', score[1])

"""MLP + GLOVE"""

model_mlp_glove = Sequential()
embedding_layer = Embedding(vocabsize, 100, weights=[embedding_matrix], input_length=maxlength, trainable=False)
model_mlp_glove.add(embedding_layer)
model_mlp_glove.add(Flatten())
model_mlp_glove.add(Dense(16, activation='sigmoid'))
model_mlp_glove.add(Dense(3, activation='softmax'))
model_mlp_glove.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
print(model_mlp_glove.summary())

history_mlp_glove = model_mlp_glove.fit(X_train, Y_train, batch_size=16000, epochs=250, verbose=1, validation_split=0.2)
score = model_mlp_glove.evaluate(X_test, Y_test, verbose=1)
scoretrain = model_mlp_glove.evaluate(X_train, Y_train, verbose=1)
from sklearn.metrics import classification_report

y_pred = model_mlp_glove.predict(X_test, batch_size=64, verbose=1)
y_pred_ = numpy.argmax(y_pred, axis=1)

print(classification_report(nn11, y_pred_))

print('Train Accuracy:', scoretrain[1])
print('Test Accuracy:', score[1])

"""CNN + Trainable Embedding"""

model_cnn = Sequential()
embedding_layer = Embedding(vocabsize, 100, input_length=maxlength)
model_cnn.add(embedding_layer)
model_cnn.add(Conv1D(512, 5, activation='relu'))
model_cnn.add(GlobalMaxPooling1D())
model_cnn.add(Dense(16, activation='sigmoid'))
model_cnn.add(Dense(3, activation='softmax'))
model_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
print(model_cnn.summary())

history_cnn = model_cnn.fit(X_train, Y_train, batch_size=16000, epochs=35, verbose=1, validation_split=0.2)
score = model_cnn.evaluate(X_test, Y_test, verbose=1)
scoretrain = model_cnn.evaluate(X_train, Y_train, verbose=1)
from sklearn.metrics import classification_report

y_pred = model_cnn.predict(X_test, batch_size=64, verbose=1)
y_pred_ = numpy.argmax(y_pred, axis=1)

print(classification_report(nn11, y_pred_))
print('Train Accuracy:', scoretrain[1])
print('Test Accuracy:', score[1])

"""CNN + GloVe"""

model_cnn_glove = Sequential()
embedding_layer = Embedding(vocabsize, 100, weights=[embedding_matrix], input_length=maxlength, trainable=False)
model_cnn_glove.add(embedding_layer)
model_cnn_glove.add(Conv1D(512, 5, activation='relu'))
model_cnn_glove.add(GlobalMaxPooling1D())
model_cnn_glove.add(Dense(16, activation='sigmoid'))
model_cnn_glove.add(Dense(3, activation='softmax'))
model_cnn_glove.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
print(model_cnn_glove.summary())

history_cnn_glove = model_cnn_glove.fit(X_train, Y_train, batch_size=16000, epochs=150, verbose=1, validation_split=0.2)
score = model_cnn_glove.evaluate(X_test, Y_test, verbose=1)
scoretrain = model_cnn_glove.evaluate(X_train, Y_train, verbose=1)
from sklearn.metrics import classification_report

y_pred = model_mlp_glove.predict(X_test, batch_size=64, verbose=1)
y_pred_ = numpy.argmax(y_pred, axis=1)

print(classification_report(nn11, y_pred_))

print('Train Accuracy:', scoretrain[1])
print('Test Accuracy:', score[1])

"""Model 
Pickling
"""

m1 = 'mlp' + '.joblib'
m2 = 'mlp_glove' + '.joblib'
m3 = 'cnn' + '.joblib'
m4 = 'cnn_glove' + '.joblib'
h1 = 'history_mlp' + '.joblib'
h2 = 'history_mlp_glove' + '.joblib'
h3 = 'history_cnn' + '.joblib'
h4 = 'history_cnn_glove' + '.joblib'

joblib.dump(model_mlp, m1)
joblib.dump(model_mlp_glove, m2)
joblib.dump(model_cnn, m3)
joblib.dump(model_cnn_glove, m4)
joblib.dump(history_mlp, h1)
joblib.dump(history_mlp_glove, h2)
joblib.dump(history_cnn, h3)
joblib.dump(history_cnn_glove, h4)

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)  

# get the folder id where you want to save your file
file = drive.CreateFile({'parents':[{u'id': '1rmTGbb19iJn6VbHoHdTYQbK0m0V_EQvP'}]})
file.SetContentFile(m1)
file.Upload() 

file = drive.CreateFile({'parents':[{u'id': '1rmTGbb19iJn6VbHoHdTYQbK0m0V_EQvP'}]})
file.SetContentFile(m2)
file.Upload() 

file = drive.CreateFile({'parents':[{u'id': '1rmTGbb19iJn6VbHoHdTYQbK0m0V_EQvP'}]})
file.SetContentFile(m3)
file.Upload() 

file = drive.CreateFile({'parents':[{u'id': '1rmTGbb19iJn6VbHoHdTYQbK0m0V_EQvP'}]})
file.SetContentFile(m4)
file.Upload() 

file = drive.CreateFile({'parents':[{u'id': '1rmTGbb19iJn6VbHoHdTYQbK0m0V_EQvP'}]})
file.SetContentFile(h1)
file.Upload() 

file = drive.CreateFile({'parents':[{u'id': '1rmTGbb19iJn6VbHoHdTYQbK0m0V_EQvP'}]})
file.SetContentFile(h2)
file.Upload()

file = drive.CreateFile({'parents':[{u'id': '1rmTGbb19iJn6VbHoHdTYQbK0m0V_EQvP'}]})
file.SetContentFile(h3)
file.Upload()

file = drive.CreateFile({'parents':[{u'id': '1rmTGbb19iJn6VbHoHdTYQbK0m0V_EQvP'}]})
file.SetContentFile(h4)
file.Upload()

# model_mlp = joblib.load('/content/drive/My Drive/DATASET FOR ML ASSIGNMENT/' + m1)
# model_mlp_glove = joblib.load('/content/drive/My Drive/DATASET FOR ML ASSIGNMENT/' + m2)
# model_cnn = joblib.load('/content/drive/My Drive/DATASET FOR ML ASSIGNMENT/' + m3)
# model_cnn_glove = joblib.load('/content/drive/My Drive/DATASET FOR ML ASSIGNMENT/' + m4)
# history_mlp = joblib.load('/content/drive/My Drive/DATASET FOR ML ASSIGNMENT/' + h1)
# history_mlp_glove = joblib.load('/content/drive/My Drive/DATASET FOR ML ASSIGNMENT/' + h2)
# history_cnn = joblib.load('/content/drive/My Drive/DATASET FOR ML ASSIGNMENT/' + h3)
# history_cnn_glove = joblib.load('/content/drive/My Drive/DATASET FOR ML ASSIGNMENT/' + h4)

"""# Plotting the Graphs"""

import matplotlib.pyplot as plt

"""NN + Self Trained Embedding Layer"""

plt.plot(list(range(1, 101)), history_mlp.history['val_loss'], color='green', label='Validation Loss')
plt.plot(list(range(1, 101)), history_mlp.history['loss'], color='brown', label='Training Loss')
plt.plot(list(range(1, 101)), history_mlp.history['val_acc'], color='indigo', label='Validation Accuracy')
plt.plot(list(range(1, 101)), history_mlp.history['acc'], color='orange', label='Training Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Loss/Accuracy')
plt.title('Model Loss & Accuracy (NN with Self Training Embedding Layer)')
plt.legend(loc='best')
plt.show()

"""NN + GloVe"""

plt.plot(list(range(1, 251)), history_mlp_glove.history['val_loss'], color='green', label='Validation Loss')
plt.plot(list(range(1, 251)), history_mlp_glove.history['loss'], color='brown', label='Training Loss')
plt.plot(list(range(1, 251)), history_mlp_glove.history['val_acc'], color='indigo', label='Validation Accuracy')
plt.plot(list(range(1, 251)), history_mlp_glove.history['acc'], color='orange', label='Training Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Loss/Accuracy')
plt.title('Model Loss & Accuracy (NN with GloVe Embedding Layer)')
plt.legend(loc='best')
plt.show()

"""CNN + Self Trained Embedding Layer"""

plt.plot(list(range(1, 36)), history_cnn.history['val_loss'], color='green', label='Validation Loss')
plt.plot(list(range(1, 36)), history_cnn.history['loss'], color='brown', label='Training Loss')
plt.plot(list(range(1, 36)), history_cnn.history['val_acc'], color='indigo', label='Validation Accuracy')
plt.plot(list(range(1, 36)), history_cnn.history['acc'], color='orange', label='Training Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Loss/Accuracy')
plt.title('Model Loss & Accuracy (CNN with Self Training Embedding Layer)')
plt.legend(loc='best')
plt.show()

"""CNN + GloVe"""

plt.plot(list(range(1, 151)), history_cnn_glove.history['val_loss'], color='green', label='Validation Loss')
plt.plot(list(range(1, 151)), history_cnn_glove.history['loss'], color='brown', label='Training Loss')
plt.plot(list(range(1, 151)), history_cnn_glove.history['val_acc'], color='indigo', label='Validation Accuracy')
plt.plot(list(range(1, 151)), history_cnn_glove.history['acc'], color='orange', label='Training Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Loss/Accuracy')
plt.title('Model Loss & Accuracy (CNN with GloVe Embedding Layer)')
plt.legend(loc='best')
plt.show()